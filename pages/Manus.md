---
title: "Manus"
---

![image](https://gyazo.com/c126c852a451f3cfc3afae5d4d25999f/thumb/1000)
2025-05-03
I could use it, but I don't really know what to use it for.
- Try [/mtane0412/Manus](https://scrapbox.io/mtane0412/Manus).

The statement "exceeded OpenAI Deep Research in the GAIA benchmark" is all by itself, but I don't know what it means.
- > GAIA is a benchmark for evaluating general-purpose AI assistants in real-world problem solving.
- <img src='https://scrapbox.io/api/pages/nishio-en/o3/icon' alt='o3.icon' height="19.5"/>The feature of this method is that it goes against the existing "hard quiz type" such as MMLU and ARC, and measures the gap with a practical task that is "easy for humans".
    - valuation index
        - Accuracy (percentage of correct answers): Scores each question according to whether or not it is an exact match.
        - Cost metrics: Performance-cost curve with execution costs such as API fees

Benchmark that evaluates the latter of "agents who are smart but messy and make mistakes from time to time" and "agents who are not smart but do not make mistakes and do the task well with the answer.
- That's not what OpenAI is aiming for, which is trying to increase emergence even if it means more halcyonation.
        - [[o3 is suitable for tasks with no correct answer because of many halcyonations]]
- Well, to be a stable component of society, it's important not to make mistakes.

---
This page is auto-translated from [/nishio/Manus](https://scrapbox.io/nishio/Manus) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.