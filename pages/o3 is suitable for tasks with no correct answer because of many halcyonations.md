---
title: "o3 is suitable for tasks with no correct answer because of many halcyonations"
---


- I wrote this in a chat with [[Takahiro Yasuno considering running for the House of Councillors]], but I'm starting to think it's important.
- [[o3]] is suitable for [[tasks with no correct answer]] because it has a lot of [[halcination]].
- If you try to suppress halucinations, you train them to [[say they don't know what they don't know]].
- If you do that, when you try something new, you get to say, "I don't know."
    - [[Creative tasks]] [[do not know the correct answer in advance]].
    - [[moving in a direction that looks good]].
    - This "looks good" [[Polanyi's tacit knowledge]].
- There's no way OpenAI isn't measuring halcyonation.
    - So why have you come up with a model with increased halcination, because you've decided that's not an appropriate KPI.

---
This page is auto-translated from [/nishio/o3はハルシネーションが多いので正解のないタスク向き](https://scrapbox.io/nishio/o3はハルシネーションが多いので正解のないタスク向き) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.