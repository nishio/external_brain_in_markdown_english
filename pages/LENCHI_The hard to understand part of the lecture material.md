---
title: "LENCHI_The hard to understand part of the lecture material"
---

2024-12-08 The following was generated by ChatGPT o1 pro mode

---
[[Points made by o1 on the presentation materials (points that are difficult to understand and suggestions for improvement)]].

# RAG（Retrieval-Augmented Generation）
Pointed out: it's hard to tell what the abbreviation RAG alone refers to.
Supplemental Proposal:
RAG is a technique whereby the LLM retrieves information from an external database (Retrieval) and uses the results to generate output (Generation). This allows the LLM to dynamically incorporate needed information from general knowledge as well as specific domain and personalized data to return more accurate responses."

# LLM for Commodities
.
Pointed out: some people are not familiar with the term "commodity".
Supplemental Proposal:
The term "commodity LLM" refers to a standard LLM that is available in a generic state, with no special adjustments or additional learning. Since it is difficult to differentiate by simply using the same model for everyone, individual knowledge bases must be combined to create uniqueness.

# Zoom function (e.g. Kozaneba)
.
Pointed out: unclear what the zoom feature brings
Supplemental Proposal:
The zoom feature allows you to look at a group of ideas from a bird's eye view or zoom in on a specific area to examine it in more detail. This makes it easier to move back and forth between the big picture and partial details, facilitating idea organization and problem solving."

# Broadcasting and Broad Listening
Pointed out: contrast is difficult to understand at first glance
Supplemental Proposal:
Broadcasting refers to the transmission of information to a large number of people in one direction, e.g., television or radio broadcasts. Broad listening, on the other hand, is a new form of communication that effectively summarizes and organizes the statements and opinions of many people to enable interactive understanding and dialogue."

# Context Window
.
Pointed out: terminology is unfamiliar.
Supplemental Proposal:
The "context window refers to the maximum amount of text that the LLM can reference at one time. The larger it is, the more consistent context it can handle long discussions and large amounts of text, allowing for more complex requests and longer project memory-keeping interactions."

[Speech materials (archive)
next:  [[Intellectual Production Techniques for Engineers Using LLM (Presentation Material)]]
next next:  [[LENCHI_Real-time discussion among the audience of the lecture]]

---
This page is auto-translated from [/nishio/LENCHI_講演資料のわかりにくいところ](https://scrapbox.io/nishio/LENCHI_講演資料のわかりにくいところ) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.