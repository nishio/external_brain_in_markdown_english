---
title: "Phi-3"
---

> [hillbig](https://twitter.com/hillbig/status/1782902418449326325) Phi-3 3B, 7B trained on high-quality training data created by filtering training data using LLM and generating textbook-like data. (After learning with general data and acquiring inference ability, high-quality data is used to introduce knowledge). It can run on a smartphone and is comparable to MMLU scores such as GPT-3.5, which is nearly 10 times larger. [https://arxiv.org/abs/2404.14219](https://arxiv.org/abs/2404.14219)
>
>  I have actually tried it and it seems to be prone to sudden performance degradation and output corruption on slightly off tasks, which was not the case with Llama3 8B and others. This may be due to the fact that the model is small, so it concentrates on the tasks (benchmarks, etc.) that were targeted at the time of training data design, and allocates memory capacity to them.
>  Nevertheless, it is astounding that the model sizes that achieve the same performance are getting smaller at a fraction of the rate each year. As expected, a cycle is taking place in which those with stronger generative models are also more competitive in data creation, creating stronger models arxiv.org


---
This page is auto-translated from [/nishio/Phi-3](https://scrapbox.io/nishio/Phi-3) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.