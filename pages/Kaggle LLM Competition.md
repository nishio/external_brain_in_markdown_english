---
title: "Kaggle LLM Competition"
---

> [tamaki_730](https://twitter.com/tamaki_730/status/1714060628720021510) [1st place. Single model inference | Kaggle [https://www.kaggle.com/](https://www.kaggle.com/) code/ybabakhin/1st-place-single-model-inference/notebook] The 1st place solution of the LLM competition is amazing. The first place score was obtained just by averaging the inferences from two different embeddings in one model.

> [tamaki_730](https://twitter.com/tamaki_730/status/1714060736127725625) I'm reading the weights separately for the head, but how do you do that when learning?

> [yume_neko92](https://twitter.com/yume_neko92/status/1714976084796821752) It took me a little while because I was doing a lot of work, but I read through all the solutions for the LLM competition.
>  This is mostly a memorandum for myself, but I published the organized contents if anyone is interested.
>  [kaggle LLM Competition Top Solutions Summary](https://zenn.dev/yume_neko/articles/7347ba6b081e93)

> [yume_neko92](https://twitter.com/yume_neko92/status/1714976866510193001) When I read the article, I thought that Retrieval was important, and that the teams that did a good job of collecting good quality wiki articles and devising a way to search them with high accuracy were the strongest. I thought that Retrieval was important, and the teams that were doing a good job of collecting good quality wiki articles and devising a good search method for them were strong.
>
>  On top of that, the higher-ranked teams were using the LLM model well, combining and improving tasks well, and devising various other things.

---
This page is auto-translated from [/nishio/Kaggle LLMコンペ](https://scrapbox.io/nishio/Kaggle LLMコンペ) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.