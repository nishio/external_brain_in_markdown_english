---
title: "Order-Embeddings of Images and Language"
---

> [tksmatsubara](https://x.com/tksmatsubara/status/1821306286656647394) In a combination of logic-like stuff and deep learning, I like the idea of embedding in [[ordered vector space]] to create the structure of [[distribution bundles]]. I much prefer order embedding (ICLR2016), which represents [[implication]] of sentences and images. order-Embeddings of Images and Language

Order-Embeddings of Images and Language
Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.
[https://arxiv.org/abs/1511.06361](https://arxiv.org/abs/1511.06361)

---
This page is auto-translated from [/nishio/Order-Embeddings of Images and Language](https://scrapbox.io/nishio/Order-Embeddings of Images and Language) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.