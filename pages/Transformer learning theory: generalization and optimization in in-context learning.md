---
title: "Transformer learning theory: generalization and optimization in in-context learning"
---

[NLP Colloquium | Learning Theory of Transformer: Generalization and Optimization in In-context learning (Daiji Suzuki) [https://nlp-colloquium-jp.github.io/schedule/2024-09-25_taiji-](https://nlp-colloquium-jp.github.io/schedule/2024-09-25_taiji-) suzuki/]
> Taiji Suzuki (The University of Tokyo)
> This paper introduces recent theoretical studies that clarify the learning ability of [[Transformers]], mainly on the subject of [[In-context leaning]]. First, as a theory of expressive power, we show that Transformer can approximate functions with [[anisotropic smoothness]] and can learn [[autoregressive data]]. We show that similar results can be achieved using [[state-space models]]. Next, as an optimization theory, we show that [[nonlinear feature learning]] can be optimized, and that the computational efficiency can be evaluated by the [[information index]] of the true function. If time permits, we will also show that [[minimax optimality]] is satisfied in in-context learning as a statistical theory.

![image](https://gyazo.com/2467b4d2204986d5ad3c234aacda2220/thumb/1000)
[[linear note]].
- [[Linear Attention]]

---
This page is auto-translated from [/nishio/Transformerの学習理論: In-context learningにおける汎化と最適化の理論](https://scrapbox.io/nishio/Transformerの学習理論: In-context learningにおける汎化と最適化の理論) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.