---
title: "LiteLLM Proxy"
---

<img src='https://scrapbox.io/api/pages/nishio-en/o3-mini-high/icon' alt='o3-mini-high.icon' height="19.5"/>LiteLLM Proxy is a proxy server that can call the APIs of over 100 large-scale language models (LLMs) with a unified interface in [[OpenAI format]]. This allows models from multiple providers such as Azure OpenAI, Anthropic, Bedrock, and HuggingFace to be used via a single API, improving code uniformity and operational efficiency. It also includes useful operational features such as error handling, fallback, caching, token usage and cost tracking, rate limiting, etc. It can be easily deployed via Docker, etc., and can be integrated with observation tools such as Langfuse to facilitate log management and analysis. Log management and analysis can be easily performed by linking with observation tools such as Langfuse.
- [LiteLLM Proxy Server (LLM Gateway) | liteLLM](https://docs.litellm.ai/docs/simple_proxy)
- [BerriAI/liteLLM-proxy](https://github.com/BerriAI/liteLLM-proxy)

---
This page is auto-translated from [/nishio/LiteLLM Proxy](https://scrapbox.io/nishio/LiteLLM Proxy) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.