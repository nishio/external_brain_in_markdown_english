---
title: "Lost in the Middle: How Language Models Use Long Contexts"
---

> While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.
[https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)

> [@ImAI_Eruel](https://twitter.com/ImAI_Eruel/status/1677232964562980866): Token length is a topic of discussion, but in the long prompts (contexts) that are actually entered into the LLM, it is better to include important information at the beginning or at the end. Research suggests that it is better to include important information at the beginning or at the end of a long prompt (context).
> [https://t.co/gEQKR5c3y8](https://t.co/gEQKR5c3y8)
> In the experiment, there is a considerable difference in accuracy between the two positions.
> I feel that the research has once again shown that people who are familiar with the use of the product have a firm intuitive grasp of the findings.
> ![image](https://pbs.twimg.com/media/F0a5dbjaYAAXNtw.png)


---
This page is auto-translated from [/nishio/Lost in the Middle: How Language Models Use Long Contexts](https://scrapbox.io/nishio/Lost in the Middle: How Language Models Use Long Contexts) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.