---
title: "Reverse Reinforcement Learning"
---

> Dramatically valuable survey paper on inverse reinforcement learning. It describes from the basic mechanism to the applications. The typical methods of inverse reinforcement learning (Max Margin/Max Entropy/Bayesian) are well organized and written.
> arxiv.org/abs/1806.06877
[https://twitter.com/icoxfog417/status/1012664138026311680?s=21](https://twitter.com/icoxfog417/status/1012664138026311680?s=21)
[https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf)

> GAN], inverse reinforcement learning, and [[energy-based model]] can be regarded as the same thing, and techniques from other communities can be used, as long as the generated model G is given a likelihood. For example, the [[autoregressive model]] can be used to stabilize learning if the current GAN G is given a likelihood. arxiv.org/abs/1611.03852
[https://twitter.com/hillbig/status/811454974274060288?s=21](https://twitter.com/hillbig/status/811454974274060288?s=21)
[https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf)
[https://arxiv.org/abs/1611.03852](https://arxiv.org/abs/1611.03852)
- [[reinforcement learning]]

---
This page is auto-translated from [/nishio/逆強化学習](https://scrapbox.io/nishio/逆強化学習) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.