---
title: "A Framework for Constructing Concept Maps from E-Books Using Large Language Models: Challenges and Future Directions"
---

![image](https://gyazo.com/71b0a04b4c85c210d0158d1d68362415/thumb/1000)

A Framework for Constructing Concept Maps from E-Books Using Large Language Models: Challenges and Future Directions
A story about using LLM to create a [conceptual map
[https://www.researchgate.net/publication/389522283_A_Framework_for_Constructing_Concept_Maps_from_E-Books_Using_Large_Language_Models_Challenges_and_Future_Directions](https://www.researchgate.net/publication/389522283_A_Framework_for_Constructing_Concept_Maps_from_E-Books_Using_Large_Language_Models_Challenges_and_Future_Directions)

<img src='https://scrapbox.io/api/pages/nishio-en/GPT5/icon' alt='GPT5.icon' height="19.5"/>
A study that proposed a framework for automatic generation of concept maps from e-books (segmentation → concept extraction → relationship identification → integration) in LLM (GPT-4o), and tested its effectiveness and issues using elementary Python lecture materials.

summary of key points
- Background: E-books are not suitable for structured browsing. Concept maps are effective for understanding, memorizing, and navigating, but human creation is expensive; LLM aims to automate this process.
- Framework:
    1. sectioning of chapters and sections → 2) extraction of key concepts in each section → 3) identification of relationships among concepts (e.g., hierarchical relationships) → 4) overall integration into a concept map (directed graph).
- Difference from related studies: conventional TF-IDF/LDA/cooccurrence/knowledge base utilization is task fragmented. The proposal uses LLM for consistent processing and contextual understanding to suppress semantic drift.
- Assessment data: 12 lectures of the university's "Introduction to Python" (electronic materials).
- Main results:
    - Sectional split: 57 sections in total, 10/12 lectures in perfect agreement (average 4.75 sections/lecture).
    - Concept extraction: 138 GPT-generated concepts vs. 111 teacher GT concepts; Precision 0.80 / Recall 0.86 / F1 0.83. Complete coverage of GT in 6/12. Additional 27 concepts (many useful), 18 missing (may be absorbed in hierarchies or paraphrases).
    - Relation identification: 154 hierarchical relations, 70 others; hierarchies are generally appropriate, but relations with little semantic value are mixed in. Suggested that constrained generation is effective.
    - Generated maps prioritize conceptual logic over teacher sequencing order. Provides another perspective on learning and understanding.
- Issue:
    - Inconsistency with educational goals (mixing peripheral concepts).
    - Optimization of human intervention (human-in-the-loop burden).
    - Hallucinations (especially non-hierarchical relationships). Mitigated by choice constraints, etc.
- Future:
    - Domain adaptation/fine-tuning to align with educational design.
    - Concept stratification (core/auxiliary/developmental) and expansion.
    - Co-generated by role-sharing agents (administrators, domain experts, and data engineers).
    - E-book system integration (relationship discovery and dashboarding with interactive navigation, user feedback, and learning logs).

Conclusion.
LLM is promising for automatic concept map generation (especially hierarchical structure), but educational goal alignment, relationship quality control, and human intervention design are key. Validation with large-scale data and other LLMs is the next step.

[[pConceptMap2025-09-08]]
---
This page is auto-translated from [/nishio/A Framework for Constructing Concept Maps from E-Books Using Large Language Models: Challenges and Future Directions](https://scrapbox.io/nishio/A Framework for Constructing Concept Maps from E-Books Using Large Language Models: Challenges and Future Directions) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.