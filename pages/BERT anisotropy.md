---
title: "BERT anisotropy"
---

> Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.
- Ethayarajh, Kawin. How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pages 55–65.
- (DeepL) Replacing static word embeddings with contextualized word representations has shown significant improvements in many natural language processing tasks. But how contextual are the contextualized representations produced by models such as ELMo and BERT? Does each word have an infinitely large number of context-specific representations, or are words essentially assigned one of a finite number of word sense representations? For one thing, the context-specific representations of all words are not isotropic in any layer of the context-specific model. A representation of the same word in different contexts has greater cosine similarity than a representation of two different words, but this self-similarity is much lower in the upper layers. This suggests that the upper layers of the contextualization model produce more context-specific representations, just as the upper layers of LSTM produce more task-specific representations. in all layers of ELMo, BERT, and GPT-2, on average, of the variance in the contextualized representation of a word, the static embedding of that word explained by the static embedding is less than 5%, providing some justification for the success of the contextualized representation.
- [https://aclanthology.org/D19-1006/](https://aclanthology.org/D19-1006/)

---
This page is auto-translated from [/nishio/BERTの異方性](https://scrapbox.io/nishio/BERTの異方性) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.