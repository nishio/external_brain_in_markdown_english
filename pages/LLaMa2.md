---
title: "LLaMa2"
---

> [@goto_yuta_](https://twitter.com/goto_yuta_/status/1684732906005241856?s=20): now that you mention it, the LLaMA2 paper is a Godsend and has too much learning value.
> [[RLHF]] describes the contents of the data used for [[RLHF]] and how the output changes before and after RLHF for safety, and anyway, it is super detailed about RLHF.
> ![image](https://pbs.twimg.com/media/F2Fd7UraEAADBjS.png)![image](https://pbs.twimg.com/media/F2Fd7Upa8AAKM-8.jpg)

> [@goto_yuta_](https://twitter.com/goto_yuta_/status/1684454407734546432?s=20): the fine-tuned model of LLaMA2 on the Open LLM Leaderboard scored 70.6 and won the GPT3.5 with a score of 70.
> An accomplishment that will go down in the history of open models!
---
This page is auto-translated from [/nishio/LLaMa2](https://scrapbox.io/nishio/LLaMa2) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.