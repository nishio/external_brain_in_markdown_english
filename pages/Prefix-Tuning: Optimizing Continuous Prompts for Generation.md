---
title: "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
---

> Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.
[https://arxiv.org/abs/2101.00190](https://arxiv.org/abs/2101.00190)
(DeepL) Fine tuning is a de facto method for performing downstream tasks by leveraging large, pre-trained language models. However, because fine tuning changes the parameters of all language models, it is necessary to store a complete copy for each task. In this paper, we propose a lightweight alternative to fine tuning for natural language generation tasks: prefix tuning, which optimizes a small continuous vector (called a prefix) that is task-specific while keeping the language model parameters frozen. Prefix tuning is inspired by prompts and allows us to focus on this prefix as if subsequent tokens were "virtual tokens". We apply prefix tuning to GPT-2 for table-to-text generation and BART for summarization. By learning only 0.1% of the parameters, we find that prefix-tuning obtains comparable performance in all data settings, outperforms fine-tuning in low data settings, and extrapolates better to examples containing unseen topics during training.

---
This page is auto-translated from [/nishio/Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://scrapbox.io/nishio/Prefix-Tuning: Optimizing Continuous Prompts for Generation) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.