---
title: "Augmenting Textual Generation via Topology Aware Retrieval"
---

[https://arxiv.org/abs/2405.17602](https://arxiv.org/abs/2405.17602)

<img src='https://scrapbox.io/api/pages/nishio-en/GPT5/icon' alt='GPT5.icon' height="19.5"/>What paper?
[[Topo-RAG]] (Topology-aware Retrieval-Augmented Generation). Based on the assumption that texts are connected by graphs due to citation, co-subscription, email traffic, etc., we propose a method to retrieve and augment generation not only by word sense proximity but also by topological proximity of the graphs. The target is not QA but text generation (summarization and addition). ([arXiv](https://arxiv.org/html/2405.17602v1))
- <img src='https://scrapbox.io/api/pages/nishio-en/nishio/icon' alt='nishio.icon' height="19.5"/>Seems appropriate for data with a link structure like Cosense.

Core Idea (two types of "proximity")
- Proximity-based (proximity): nodes that can be reached in fewer hops on the graph are considered close. Implementation is the embedded cosine similarity obtained by **diffusion**. The computation is inherently heavy, but random projection speeds it up. ([arXiv](https://arxiv.org/html/2405.17602v1))
- Role-based (role/structure): nodes whose local structures have similar roles (e.g., employees with the same job title). Implementation is L2 distance with structure embedding obtained from GraphWave. ([arXiv](https://arxiv.org/html/2405.17602v1))
First, we demonstrate that there is a positive correlation between textual similarity and topological similarity (confirmed on multiple datasets of articles, reviews, and emails). Then, topologically similar documents are obtained Top-K, inserted into the prompt, and generated by LLM. ([arXiv](https://arxiv.org/html/2405.17602v1))

Framework (Topo-RAG)
1. precomputed topological similarity for all node pairs (space sparsified by Top-K)
2. prompt the beginning word (partially observed text) of the node to be generated + phase Top-K document.
3. generate the rest of the text in LLM (GPT-3/3.5 in the paper). ([arXiv](https://arxiv.org/html/2405.17602v1))

Experimental Design
- 9 datasets / 3 domains: Citation (Cora, PubMed, ArXiv), E-commerce (Amazon Product/Book/Epinion/Music/Pantry), Social (Enron-Email). ([arXiv](https://arxiv.org/html/2405.17602v1))
- Comparison: None (no addition) / Random / Text (search by semantic similarity) / Topo (this method).
- Metrics: BLEU-4 / ROUGE-L / BERT-F1 plus downstream performance with node classification / link prediction. ([arXiv](https://arxiv.org/html/2405.17602v1))

Main Results
- Average performance (GPT-3.5): BLEU +61.8%, ROUGE +7.6%, BERT-F1 +1.0% compared to Text.
Average performance (GPT-3): BLEU +51.3%, ROUGE +6.2%, BERT-F1 +1.2% compared to Text. ([arXiv](https://arxiv.org/html/2405.17602v1))
- Top-RAG is consistently best in downstream tasks (e.g., classification 71.09 to 75.49% for Cora/GCN, link prediction 77.15 to 89.12). ([arXiv](https://arxiv.org/html/2405.17602v1))
- The fewer the opening words, the greater the gain of Topo-RAG (the less information, the stronger the phase completion). ([arXiv](https://arxiv.org/html/2405.17602v1))

Differences from Existing Laws
KG-RAG, which adds knowledge graph-derived triple neighborhoods, is more QA-centric, whereas Topo-RAG is new in that it uses the phase patterns (proximity/roles) of the graph itself to effect generation. ([arXiv](https://arxiv.org/html/2405.17602v1))

Mounting Precautions
- Precomputation of phase similarity is heavy (all-to-all). This paper assumes operation with Top-K sparsification + offline computation. The long-form handling is used in conjunction with existing long-form measures. ([arXiv](https://arxiv.org/html/2405.17602v1))

Where does it work?
- Paper writing support (citation network) / EC review generation (co-purchase, same user consolidation) / Internal mail and FAQ (role similarity by department/position). Tends to be stronger for style and terminology merging than RAGs with semantic similarity only. ([arXiv](https://arxiv.org/html/2405.17602v1))

Limitations/notes
- Difficult to use in areas with no/thin graphs.
- BERT-F1 extension is smaller (more effective for vocabulary and phrase agreement).
- For data with weak correlations (e.g., Epinion), gains can also be relatively small. ([arXiv](https://arxiv.org/html/2405.17602v1))
If necessary, we can provide a minimum implementation sample from phase features (diffusion/GraphWave) to Top-K acquisition to prompt design with the data at hand.


---
This page is auto-translated from [/nishio/Augmenting Textual Generation via Topology Aware Retrieval](https://scrapbox.io/nishio/Augmenting Textual Generation via Topology Aware Retrieval) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.