---
title: "Fine-tuning language models to find agreement among humans with diverse preferences"
---

> Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single "generic" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.
- [2211.15006 Fine-tuning language models to find agreement among humans with diverse preferences](https://arxiv.org/abs/2211.15006)

(DeePL)
- Recent research in large-scale language modeling (LLM) has fine-tuned output to align it with prototypical user preferences. This research assumes that human preferences are static and homogeneous across individuals, and that a more general alignment can be achieved by matching to a single "generic" user.
- Here, we examine a different issue: how machines can help people with diverse opinions find unity by embracing [[the heterogeneity of human preferences]].
        - [[People have personalities.]]
- We fine-tune the 70 billion-parameter LLM to generate statements that maximize expected agreement for potentially diverse groups of people with diverse opinions.
- Human participants write opinions on thousands of questions touching on moral and political issues (e.g., "Should taxes be raised on the rich?") and rate the agreement and quality of the candidate consensus statements generated by the LLM.
- We then train a reward model that predicts individual preferences, allowing us to quantify and rank consensus statements in terms of their appeal to the whole group, defined according to different [[aggregate functions]] ([[social welfare functions]]).
    - [Social welfare function - Wikipedia](https://en.wikipedia.org/wiki/Social_welfare_function)
    - [Social welfare function - Wikipedia](https://ja.wikipedia.org/wiki/社会的厚生関数)
- The model generates consensus statements that are preferred by human users over those of the prompt LLM (>70%), significantly outperforming the tightly fine-tuned baseline, which lacks a final ranking step. Furthermore, consensus statements in our best model are preferred over the best human-generated opinions (>65%).
- When consensus statements were silently generated from only a subset of group members, the excluded members were more likely to disagree, indicating that the consensus statements were influenced by the contribution of the individuals. These results demonstrate the potential of using LLM to help human groups align their values with each other.

---
This page is auto-translated from [/nishio/Fine-tuning language models to find agreement among humans with diverse preferences](https://scrapbox.io/nishio/Fine-tuning language models to find agreement among humans with diverse preferences) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.