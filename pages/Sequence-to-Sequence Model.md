---
title: "Sequence-to-Sequence Model"
---

- Also [[seq2seq]], [[Encoder-Decoder]], and [[Sequence transformation model]].
- [1409.3215 Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)(2014)
    - If we use the [[self-caution]] mechanism, isn't there much need to use the Encoder-Decoder configuration? I'm starting to get the feeling that it is (2018-10-17)


- [[Pointer Networks]]
    - [1506.03134 Pointer Networks](https://arxiv.org/abs/1506.03134)(2015)
    - It is possible to use the pointer to copy from the input
    - [[CopyNet]]
        - [1603.06393 Incorporating Copying Mechanism in Sequence-to-Sequence Learning](https://arxiv.org/abs/1603.06393)
    - [[Pointer Sentinel Mixture Models]]
        - [1609.07843 Pointer Sentinel Mixture Models](https://arxiv.org/abs/1609.07843)
    - [[Pointer-Generator Network]] (2017)
        - [1704.04368 Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)
[https://qiita.com/ymym3412/items/c84e6254de89c9952c55](https://qiita.com/ymym3412/items/c84e6254de89c9952c55)

---
This page is auto-translated from [/nishio/Sequence-to-Sequenceモデル](https://scrapbox.io/nishio/Sequence-to-Sequenceモデル) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.