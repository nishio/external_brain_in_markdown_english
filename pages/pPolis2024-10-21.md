---
title: "pPolis2024-10-21"
---

from [[pPolis2024-05-27]]
pPolis2024-10-21
2024-10-21
:

```
[f"{x * 100:.1f}%" for x in pca.explained_variance_ratio_]
```

:

```
['29.1%', '18.8%', '7.5%', '5.1%', '3.2%', '3.0%', '2.4%', '2.1%', '2.0%', '1.7%', '1.6%', '1.5%', '1.3%', '1.3%', '1.2%', '1.2%', '1.0%', '1.0%', '1.0%', '0.9%', '0.9%', '0.8%', '0.8%', '0.8%', '0.8%', '0.8%', '0.7%', '0.7%', '0.7%', '0.6%', '0.6%', '0.6%', '0.6%', '0.6%', '0.5%', '0.5%', '0.5%', '0.4%', '0.4%', '0.4%', '0.4%', '0.0%']
```

len = 42

> >> pca.n_features_in_
592

> >>pca.n_samples_
42

Huh.
- I thought you were being PCA after the transposition, but that's what I thought.
- <img src='https://scrapbox.io/api/pages/nishio-en/gpt/icon' alt='gpt.icon' height="19.5"/>
    - No transposition: Calculates the covariance between questions and shows how much each question contributes to the new basis (principal components). This is useful for exploring the relationship between questions.
    - When transposing: Calculates the covariance between respondents and shows how much each respondent contributes to the principal components. This is useful for clustering respondents and finding patterns.
- You want to cluster respondents, so that's correct.

P=592 (people)
Q=42 (questions)
> embedding.shape # (592, 2)

<img src='https://scrapbox.io/api/pages/nishio-en/gpt/icon' alt='gpt.icon' height="19.5"/>In order to consider how the answer to a given question i would contribute to the X-axis after a PCA using covariance among respondents by transposition, we first need to see how that question i affects the principal components of the respondents as a whole.

![image](https://gyazo.com/c293eda93f7f43d71acbada073db47fd/thumb/1000)
I think you're right.

![image](https://gyazo.com/61b2fcb02c10d7017e2826609e7a2fc3/thumb/1000)
Aside from which is the column, I think it's right.

![image](https://gyazo.com/ce35f6cf32917e58a8e38fab39f52139/thumb/1000)

py

```
# Obtain PCA components
matrix_centered = matrix - matrix.mean(axis=0) # Mean centered
# pca.fit(matrix_centered.T) # Perform PCA based on respondents

# 1st principal component (X-axis)
pc1 = pca.components_[0] # 1st principal component vector (weights per respondent)

for col in matrix.columns:
    question_vector = matrix_centered[col]
    # contributions[col] = np.dot(question_vector, pc1)
    c = np.dot(question_vector, pc1)

    print(f"{col}: {c:.2f}: {readable.get(col, col)}")
```

Data is ready.

- [[An experiment to let AI explain the principal component axis]]

---
This page is auto-translated from [/nishio/pPolis2024-10-21](https://scrapbox.io/nishio/pPolis2024-10-21) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.