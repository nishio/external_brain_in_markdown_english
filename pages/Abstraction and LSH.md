---
title: "Abstraction and LSH"
---

2018-10-09
    - [[Abstraction is dimensionality reduction]]
        - [[Dimensionality reduction is an abstraction]]
- To begin with, there is no way that the brain is "calculating the inner product in order and looking for the closest one" to find vectors with a high degree of similarity.
    - [[LSH]]?
        - [https://www.slideshare.net/mobile/yaruki_nil/lsh](https://www.slideshare.net/mobile/yaruki_nil/lsh)
        - [[Boltzmann machine]]?
    - Asociatron?
        - [http://www.sist.ac.jp/~kanakubo/research/neuro/associatron.html](http://www.sist.ac.jp/~kanakubo/research/neuro/associatron.html)

- Rather, is a neural net (if realized on a device capable of parallel computation) a device that can efficiently compute probabilistic Near Neighbor
    - The cost is just too high in making it happen with computers.
        - [http://people.csail.mit.edu/indyk/mmds.pdf](http://people.csail.mit.edu/indyk/mmds.pdf)

2023-12-01
- [[HNSW]] is a popular algorithm that can efficiently compute [approximate NearestNeighbors

---
This page is auto-translated from [/nishio/抽象化とLSH](https://scrapbox.io/nishio/抽象化とLSH) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.