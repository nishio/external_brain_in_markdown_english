---
title: 'Decoupling "Understanding" from "Results'
---

> [takahiroanno](https://x.com/takahiroanno/status/1884110171397664982) The human brain has an "I get it" button. If you utilize a simple saying or a specific analogy, your brain instantly feels like it [[understands]] a certain thing.
>
>  [[Explainable AI]] should be able to hack this understood button mechanism, and if the AI is smart enough, mankind won't be able to discern if it's a hack or not.

> [takahiroanno](https://x.com/takahiroanno/status/1884110396023529736) That said, I don't think we need to despair. Because [[a sense of understanding]] doesn't mean anything in the first place.
>
>  Tasks can be solved without understanding (using AI, etc.), and understanding does not necessarily mean correctness ([[conspiracy theory]]); as AI advances, more and more of the former will be true, so comprehension will become increasingly irrelevant to real-world activities

> [takahiroanno](https://x.com/takahiroanno/status/1884110479406313898) In the past, the more you understood, the better you could model the real world correctly, so you were more likely to perform the task and succeed, but that relationship is getting weaker and weaker. However, that relationship is becoming weaker and weaker as time goes on and on.

> [takahiroanno](https://x.com/takahiroanno/status/1884111018089210193) What matters is how many tasks were solved in the real world?　How many [[RESULTS]] were produced?　I think it's more important to ask "How many [[results]] did you get? These are some of the measurable aspects, and I think we can substitute a sense of understanding with these.

> [takahiroanno](https://x.com/takahiroanno/status/1884114821744140558) [[decoupling]] of "[[understanding]]" and "[[results]]"
- [[decoupling of understanding and outcomes]].

> [nishio](https://x.com/nishio/status/1884803716563472614) I think that leaving 99% of real-world problems to AI is better than humans understanding them, just like at the moment 99% of problems are "better to be generated by a compiler than written by a human machine language". On the other hand, DeepSeek has made an impact by directly tweaking [[PTX]], so there will always be "1% left".
    - [[law of leaky abstraction]]

2025-03-08
    - [[Understanding and Verification]]
    - [[AI Agents and Research]]
    - [[Fixes you don't understand]]

2025-03-10
    - [[The object of understanding is ambiguous.]]

---
This page is auto-translated from [/nishio/『理解』と『成果』のデカップリング](https://scrapbox.io/nishio/『理解』と『成果』のデカップリング) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.