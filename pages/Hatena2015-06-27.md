---
title: "Hatena2015-06-27"
---

hatena

```
<body>
*1435371890*Pyspa Diary
We began to build a language model for Python.
2038 samples, average number of tokens per sample 10491, maximum number of tokens 290226.
I have a pretty long token string, so a fixed n-gram seems like a waste.

There are 135 token species, 21380862 tokens in all.

Number of occurrences by token (partial)
>||
NAME 1086490
factor 918992
atom 911487
power 911487
term 897816
arith_expr 885288
shift_expr 884911
and_expr 884475
xor_expr 884412
||<

LogLoss with 256 hidden layers was 82.29 after 10000 cases, 82.33 in the test. it has been running loosely since then, about 82.35.

1000 mini batches
train mean loss=71.2530914678
test  mean loss=71.2531124351

I thought it was awfully big, but with two tokens the probability is almost 1. What is wrong?
</body>
```


[Hatena Diary 2015-06-27](https://nishiohirokazu.hatenadiary.org/archive/2015/06/27)
---
This page is auto-translated from [/nishio/Hatena2015-06-27](https://scrapbox.io/nishio/Hatena2015-06-27) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.