---
title: "What you see once is unknown"
---


> [hillbig](https://twitter.com/hillbig/status/1729634555609391139/history) [[LLM]] causes [[Hallucination]] even if the training data is correct. This is because a correctly calibrated predictive distribution, like [[Good-Turing estimation]], assigns the same probability to an unknown fact as to a fact observed only once during training. Showing that hallucination reduction requires prior training followed by another
>  [[Calibrated Language Models Must Hallucinate]]
>  Recent language models have a mysterious tendency to generate false but plausible-sounding text. Such "hallucinations" are an obstacle to the usability of language-based AI systems and can harm...
- [Good-Turing presumption - Wikipedia](https://ja.wikipedia.org/wiki/Good%E2%80%93Turing%E6%8E%A8%E5%AE%9A)

It could also be interpreted that [[Homo sapiens]] [[who find meaning in things they have only experienced once]] are just not [[calibrating]] enough.

- [[human bug]]

---
This page is auto-translated from [/nishio/一度だけ見たものは未知](https://scrapbox.io/nishio/一度だけ見たものは未知) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.