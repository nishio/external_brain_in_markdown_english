---
title: "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"
---

> Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design.
[https://arxiv.org/abs/2212.10559](https://arxiv.org/abs/2212.10559)
(DeepL) Large-scale pre-trained language models have shown remarkable [[in-context learning]] ([[In-context learning]], ICL) capabilities. Given a few pairs of inputs and a label demo, it can predict labels for unknown inputs without updating parameters. Despite the great success of such performance, the mechanism of its operation remains an open question. In this paper, we describe the language model as a meta-optimizer and understand in-context learning as implicit fine-tuning. Theoretically, we understand Transformer attention to have a dual form of gradient descent. We then understand ICL as follows: the GPT first generates meta-gradients according to the empirical example, and then these meta-gradients are applied to the original GPT to build the ICL model. We comprehensively compare the behavior of in-context learning and explicit fine-tuning in real tasks to provide empirical evidence supporting our understanding. Experimental results show that in-context learning behaves similarly to explicit fine-tuning from multiple perspectives.Inspired by the dual form between Transformer attention and gradient descent, we design momentum-based attention by analogy with momentum and gradient descent. The improved performance over vanilla attention further supports our understanding from another perspective and, more importantly, shows the potential to leverage our understanding for future model design.

---
This page is auto-translated from [/nishio/Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://scrapbox.io/nishio/Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at [@nishio_en](https://twitter.com/nishio_en). I'm very happy to spread my thought to non-Japanese readers.